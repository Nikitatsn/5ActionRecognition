{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import**\n"
      ],
      "metadata": {
        "id": "Y0hZVRltPwHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "m_tvZpfEYExr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rhbTqd28O-mn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models.video import r2plus1d_18\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import time\n",
        "from torchvision.io import read_video\n",
        "import av\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "video_dir = '/content/drive/MyDrive/videos/..'\n"
      ],
      "metadata": {
        "id": "zGPBO14UP3R6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание экземпляра LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(df['label'])\n",
        "df['target'] = encoded_labels\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKK4jre_P3KH",
        "outputId": "77730db8-25a5-44e7-99bb-5a1f2b2b1c1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0             name_video        label  target\n",
            "0           0  videos/video_0000.mp4  tap dancing      14\n",
            "1           1  videos/video_0001.mp4  tap dancing      14\n",
            "2           2  videos/video_0002.mp4  tap dancing      14\n",
            "3           3  videos/video_0003.mp4  tap dancing      14\n",
            "4           4  videos/video_0004.mp4  tap dancing      14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_dir, df):\n",
        "        self.video_dir = video_dir\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.video_dir, self.df['name_video'].iloc[idx])\n",
        "        video, audio, info = read_video(video_path)\n",
        "        video = self.preprocess_video(video)\n",
        "\n",
        "        target = self.df['target'].iloc[idx]\n",
        "        target = torch.tensor(target, dtype=torch.long)  # Преобразование в Tensor\n",
        "\n",
        "        return video, target\n",
        "\n",
        "    def preprocess_video(self, video):\n",
        "\n",
        "        video = video.float()\n",
        "\n",
        "\n",
        "        resize_transform = transforms.Resize((112, 112))\n",
        "        video_resized = torch.stack([resize_transform(frame.permute(2, 0, 1)).permute(1, 2, 0) for frame in video])\n",
        "\n",
        "\n",
        "        num_frames = video_resized.shape[0]\n",
        "        desired_frames = 24\n",
        "        if num_frames < desired_frames:\n",
        "\n",
        "            last_frame = video_resized[-1]\n",
        "            frames_to_add = desired_frames - num_frames\n",
        "            video_resized = torch.cat([video_resized, last_frame.unsqueeze(0).expand(frames_to_add, -1, -1, -1)], dim=0)\n",
        "        elif num_frames > desired_frames:\n",
        "\n",
        "            video_resized = video_resized[:desired_frames]\n",
        "\n",
        "\n",
        "        video_normalized = video_resized.permute(3, 0, 1, 2) / 255.0\n",
        "\n",
        "\n",
        "        video_tensor = torch.cat([frame.unsqueeze(0) for frame in video_normalized], dim=0)\n",
        "\n",
        "        return video_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "video_dataset = VideoDataset(video_dir=\"/content/drive/MyDrive/videos/..\", df=df)\n",
        "\n",
        "\n",
        "video, target = video_dataset[0]\n",
        "print(\"Video tensor shape:\", video.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w67bRHdFP27b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Определение параметров\n",
        "batch_size = 8\n",
        "validation_split = 0.2  # Доля данных, выделяемых под валидацию\n",
        "\n",
        "# Создание DataLoader для исходного датасета\n",
        "dataloader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Вычисление размера валидационного набора данных\n",
        "val_size = int(len(video_dataset) * validation_split)\n",
        "train_size = len(video_dataset) - val_size\n",
        "\n",
        "# Разделение датасета на тренировочный и валидационный наборы данных\n",
        "train_dataset, val_dataset = random_split(video_dataset, [train_size, val_size])\n",
        "\n",
        "# Создание DataLoader для тренировочного и валидационного наборов данных\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Проверка размеров полученных датасетов и батчей\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Validation dataset size:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11f9l6qY_xe",
        "outputId": "ba008080-0cc9-4d72-b424-1a0884180fde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1874\n",
            "Validation dataset size: 468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = r2plus1d_18(pretrained=True)\n",
        "\n",
        "# Определение устройства (например, GPU, если доступно)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Перенос модели на выбранное устройство\n",
        "model = model.to(device)\n",
        "\n",
        "# Проверка устройства, на котором работает модель\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBBCS6UdzWy",
        "outputId": "53f736a3-ab82-49a2-ed1a-c5c75d29356f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdmVEVNkfnVP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Sl9LUECY_nq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Определение функции потерь\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Определение learning rate оптимизатора\n",
        "optimizer_lr = 0.0001\n",
        "\n",
        "# Определение оптимизатора\n",
        "optimizer = optim.AdamW(model.parameters(), lr=optimizer_lr)\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "o5V1fcUioKbA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"---------------------epoch:{}/{}---------------------\".format(epoch+1, num_epochs))\n",
        "\n",
        "    # Train on the training data\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_dataloader)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        model = model.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs.to(device))\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_dataloader.dataset)\n",
        "    train_accuracy = 100.0 * train_correct / len(train_dataloader.dataset)\n",
        "\n",
        "    # Assign encoded labels to 'target' column in the DataFrame\n",
        "    df['target'] = encoded_labels\n",
        "\n",
        "    # Validate on the test data\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(val_dataloader.dataset)\n",
        "    test_accuracy = 100.0 * test_correct / len(val_dataloader.dataset)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Print the results after each epoch\n",
        "    print(f\"\\nepoch: {epoch+1}, lr_rate {optimizer.param_groups[0]['lr']:.4f}\")\n",
        "    print(f\"loss_train: {train_loss:.4f} | loss_valid: {test_loss:.4f}\")\n",
        "    print(f\"metric {test_accuracy}\")\n",
        "    print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")\n",
        "    print(\"-----------------------------------------------\")\n",
        "\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "13Xxq-feY_fi",
        "outputId": "41af04c1-3c24-4664-db8c-13a27988b4cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------epoch:1/10---------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/235 [03:17<12:51:57, 197.94s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aa3f1bc79cf8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}